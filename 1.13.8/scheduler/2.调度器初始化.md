---
typora-root-url: ..\..\images
typora-copy-images-to: ..\..\images
---

# hello world

```go
// mian.go
package main

import "fmt"

func main() {
	fmt.Println("hello world")
}
```

# 获取Go程序的入口函数

从`hello world`开始，首先找到程序的入口函数：

1. 在windows下编译Linux的64位可执行程序：

   ```go
   // -gcflags "-N -l" 关闭编译器的优化和函数内联
   GOOS=linux GOARCH=amd64 go build -gcflags "-N -l" -o main
   ```

2. 使用gdb进行调试，找到程序的入口函数

> window下的64位操作系统，需要通过MinGW64安装gdb；MinGW32中安装gbd去调试64位的可执行程序会出现不支持调试文件的错误

```go
gdb main   	// 进入gdb调试  
(gdb) info files
Symbols from "T:\go_1.13.7\src\func123\test\main".
Local exec file:
        `T:\go_1.13.7\src\func123\test\main', file type elf64-x86-64.
        Entry point: 0x454dc0	// 得到入口地址：0x450e20
        0x0000000000401000 - 0x000000000048cfb3 is .text
        0x000000000048d000 - 0x00000000004dc590 is .rodata
        0x00000000004dc760 - 0x00000000004dd3cc is .typelink
        0x00000000004dd3d0 - 0x00000000004dd420 is .itablink
        0x00000000004dd420 - 0x00000000004dd420 is .gosymtab
        0x00000000004dd420 - 0x0000000000548a83 is .gopclntab
        0x0000000000549000 - 0x0000000000549020 is .go.buildinfo
        0x0000000000549020 - 0x00000000005560f8 is .noptrdata
        0x0000000000556100 - 0x000000000055d150 is .data
        0x000000000055d160 - 0x00000000005789d0 is .bss
        0x00000000005789e0 - 0x000000000057b148 is .noptrbss
        0x0000000000400f9c - 0x0000000000401000 is .note.go.buildid
(gdb) b *0x454dc0	// 得到入口函数
Breakpoint 1 at 0x454dc0: file R:/go 1.13.7/go/src/runtime/rt0_linux_amd64.s, line 8.
```

在看代码之前，先引用阿波张分析调度器教程中的一段话：

> 任何一个由编译型语言（不管是C，C++，go还是汇编语言）所编写的程序在被操作系统加载起来运行时都会顺序经过如下几个阶段：
>
> 1. 从磁盘上把可执行程序读入内存；
> 2. 创建进程和主线程；
> 3. 为主线程分配栈空间；
> 4. 把由用户在命令行输入的参数拷贝到主线程的栈；
> 5. 把主线程放入操作系统的运行队列等待被调度执起来运行。

那么在创建主线程后执行go程序的第一条指令前，主线程的函数栈应该是这个样子的，其中argc和argv是内核传递给主线程的参数，argc指示argv的长度大小：

<img src="/初始化调度器前栈情况-1592836478777.jpg" alt="初始化调度器前栈情况" style="zoom:30%;" />

那么根据gdb得到的结果，能看到Go程序在启动时，就是从`runtime/rt0_linux_amd64.s`的第八行开始的，接下来进入到代码中分析。

# 调度器的初始化

```go
// runtime/rt0_linux_amd64.s
TEXT _rt0_amd64_linux(SB),NOSPLIT,$-8
    JMP _rt0_amd64(SB)

// runtime/asm_amd64.s
// _rt0_amd64 is common startup code for most amd64 systems when using
// internal linking. This is the entry point for the program from the
// kernel for an ordinary -buildmode=exe program. The stack holds the
// number of arguments and the C-style argv.
TEXT _rt0_amd64(SB),NOSPLIT,$-8
   MOVQ   0(SP), DI  // argc  	将对应值存入DI寄存器
   LEAQ   8(SP), SI  // argv	将对应内存位置存入DI寄存器
   JMP    runtime·rt0_go(SB)

```

上面几行指令将内核传递过来的参数放进DI和SI寄存器后，就跳转到`runtime·rt0_go(SB)`继续执行。而程序启动的所有逻辑都会在其中完成，包括调度器初始化、运行main方法等。下面会按照调度器初始化、执行main g 等来分段分析它。

```go
// runtime/asm_amd64.s
TEXT runtime·rt0_go(SB),NOSPLIT,$0
	// copy arguments forward on an even stack
	MOVQ	DI, AX		// argc	argv的长度
	MOVQ	SI, BX		// argv	参数地址
	SUBQ	$(4*8+7), SP		// 2args 2auto	将栈顶sp向下移动4*8+7
	ANDQ	$~15, SP		// 将sp与15相与，使栈顶sp按16字节对齐
	MOVQ	AX, 16(SP)		
	MOVQ	BX, 24(SP)
```

让栈顶SP按16字节对其，是因为CPU有一组SSE指令，这些指令使用的内存地址必须是16的倍数。

```
	// create istack out of the given (operating system) stack.
	// _cgo_init may update stackguard.
	MOVQ	$runtime·g0(SB), DI
	LEAQ	(-64*1024+104)(SP), BX
	MOVQ	BX, g_stackguard0(DI)
	MOVQ	BX, g_stackguard1(DI)
	MOVQ	BX, (g_stack+stack_lo)(DI)
	MOVQ	SP, (g_stack+stack_hi)(DI)	
```

初始化全局变量g0中几个与栈有关的全局变量，g0的主要作用主要是为runtime的运行提供一个栈，栈的地址范围为：SP-64*1024+104 ~ SP，大小大约是64K

```go
	LEAQ	runtime·m0+m_tls(SB), DI
	CALL	runtime·settls(SB)

	// store through it, to make sure it works
	get_tls(BX)				// 将线程私有变量地址取出并且放进BX
	MOVQ	$0x123, g(BX)	
	MOVQ	runtime·m0+m_tls(SB), AX		
	CMPQ	AX, $0x123
	JEQ 2(PC)
	CALL	runtime·abort(SB)
```

​		跳过CPU型号检查以及cgo初始化相关的代码，从183行继续看，这段代码主要是将thread与全局变量m0进行绑定并且验证是否成功。	

```go
// runtime/sys_linux_amd64.s
// set tls base to DI
TEXT runtime·settls(SB),NOSPLIT,$32
#ifdef GOOS_android
	// Android stores the TLS offset in runtime·tls_g.
	SUBQ	runtime·tls_g(SB), DI
#else
	// 调用时，DI中放的是m0.tls[0]的地址
    // 这里加8，主要跟ELF可执行文件格式中的TLS实现的机制有关
    // 此时DI中的值是m0.tls[1]的地址
	ADDQ	$8, DI	// ELF wants to use -8(FS)		
#endif
	// 准备arch_prctl系统调用的参数，将 m0.tls[1]的地址设置成fs段的段基址，对应fs的段寄存器
	MOVQ	DI, SI
	MOVQ	$0x1002, DI	// ARCH_SET_FS
	MOVQ	$SYS_arch_prctl, AX
	SYSCALL
	CMPQ	AX, $0xfffffffffffff001
	JLS	2(PC)
	MOVL	$0xf1, 0xf1  // crash
	RET				
```

​		通过系统调用，设置线程对应的fs段寄存器的值为`m0.tls[1]`的地址，这样线程就能通过fs寄存器找到`m0.tls`

```go
	// set the per-goroutine and per-mach "registers"
	get_tls(BX)		// 获取fs段寄存器（对应m.tls的地址）并放入到BX寄存器中 
	LEAQ	runtime·g0(SB), CX	// g0的地址 放入到CX寄存器
	MOVQ	CX, g(BX)		//  m0.tls[0] = g0
	LEAQ	runtime·m0(SB), AX	// 将m0的地址放入到AX寄存器中

	// save m->g0 = g0
	MOVQ	CX, m_g0(AX)		// m0.g0 = g0
	// save m0 to g0->m
	MOVQ	AX, g_m(CX)			// g0.m = m0
```

​		当前线程的fs段寄存器上放的是m0.tls的内存地址，将g0的内存地址存放到m0.tls中，这样就将g0和当前线程绑定在一起，之后当前线程可以通过`getg()方法`获取到g0，随后m0 和 g0 再互相引用

```go
	MOVL	16(SP), AX		// copy argc
	MOVL	AX, 0(SP)
	MOVQ	24(SP), AX		// copy argv
	MOVQ	AX, 8(SP)
	CALL	runtime·args(SB)	// 处理参数
	CALL	runtime·osinit(SB)	// 获取CPU的核数并放在全局变量ncpu中
	CALL	runtime·schedinit(SB)	// 调度初始化,完成g0、p、m0、thread 的关联
```

​		处理程序启动时传进的参数，初始化ncpu = CPU核数，调用函数初始化`schedt`，接下来先分析`runtime·schedinit(SB)`

```go
// runtime/proc.s
// The bootstrap sequence is:
//
//	call osinit
//	call schedinit
//	make & queue new G
//	call runtime·mstart
//
// The new G calls runtime·main.
func schedinit() {
	// raceinit must be the first call to race detector.
	// In particular, it must be done before mallocinit below calls racemapshadow.
    _g_ := getg()	// 获取当前线程绑定的g,这里是g0
	if raceenabled {
		_g_.racectx, raceprocctx0 = raceinit()
	}

	sched.maxmcount = 10000  // 设置最大的m数量

	······
    
	mcommoninit(_g_.m)		// 初始化m0，主要是初始化id变量以及关联allm，组成m的链表
    
	······ // 略过初始化gc等代码

	sched.lastpoll = uint64(nanotime())
	procs := ncpu
	if n, ok := atoi32(gogetenv("GOMAXPROCS")); ok && n > 0 {
		procs = n
	}
    // 1. 初始化全局变量allp（p的数组）
    // 2. 初始化procs数量的p
    // 3. 将m0和p进行绑定，并且将绑定后的p的状态设置成 _Prunning，
    // 4. 修改全局变量gomaxprocs = procs
	if procresize(procs) != nil {   // 返回有局部可运行gorountine队列的p，这里返回nil
		throw("unknown runnable goroutine during bootstrap")
	}

	······
}
```

​		在m内部，通过 `mp.alllink = allm`实现一个链表，`alllink `指向的m是上一次创建的

```go

func mcommoninit(mp *m) {
   _g_ := getg()   // 前面将 g0 跟 m0.tls， fs段寄存器关联 m0.tls，这个方法就能返回g0

   ······

   lock(&sched.lock)		// 因为schedt是全局变量，这里有必要加锁
   if sched.mnext+1 < sched.mnext {
      throw("runtime: thread ID overflow")
   }
   mp.id = sched.mnext	// 程序启动，初始值为0，因此 mp.id  = 0
   sched.mnext++
   checkmcount()	// 校验  mcount() > sched.maxmcount

  ······

   // Add to allm so garbage collector doesn't free g->m
   // when it is just in a register or thread-local storage.
   mp.alllink = allm	// 实现mp链表，未防止非主线程的m被gc回收

   // NumCgoCall() iterates over allm w/o schedlock,
   // so we need to publish it safely.
   atomicstorep(unsafe.Pointer(&allm), unsafe.Pointer(mp))	// 保存mp到allm上
   unlock(&sched.lock)

  ······
}
```

​		函数`procresize`主要初始化`procs`个p，放进全局变量`allp` 中，且将m0和`allp[0]`绑定

```go
// Change number of processors. The world is stopped, sched is locked.
// gcworkbufs are not being modified by either the GC or
// the write barrier code.
// Returns list of Ps with local work, they need to be scheduled by the caller.
func procresize(nprocs int32) *p {
	old := gomaxprocs 	// old = gomaxprocs = 0 
    
	······

	// update statistics
	now := nanotime()
	if sched.procresizetime != 0 {
		sched.totaltime += int64(old) * (now - sched.procresizetime)
	}
	sched.procresizetime = now

	······

	// 初始化 nprocs 个p，并且放入allp数组中
	for i := old; i < nprocs; i++ {
		pp := allp[i]
		if pp == nil {
			pp = new(p)
		}
		pp.init(i) // 进行p的初始化，状态为 _Pgcstop
		atomicstorep(unsafe.Pointer(&allp[i]), unsafe.Pointer(pp))
	}

	_g_ := getg()	// 获取 g0
	if _g_.m.p != 0 && _g_.m.p.ptr().id < nprocs {
		// continue to use the current P
		_g_.m.p.ptr().status = _Prunning
		_g_.m.p.ptr().mcache.prepareForSweep()
	} else {
		// release the current P and acquire allp[0].
		//
		// We must do this before destroying our current P
		// because p.destroy itself has write barriers, so we
		// need to do that from a valid P.
		if _g_.m.p != 0 {
			if trace.enabled {
				// Pretend that we were descheduled
				// and then scheduled again to keep
				// the trace sane.
				traceGoSched()
				traceProcStop(_g_.m.p.ptr())
			}
			_g_.m.p.ptr().m = 0
		}
		_g_.m.p = 0   	// 从这里执行	
		_g_.m.mcache = nil
        p := allp[0]	
		p.m = 0
		p.status = _Pidle	
		acquirep(p)		// 取 allp[0] 与 m0 进行绑定，并且修改p的状态为：_Prunning
		if trace.enabled {
			traceGoStart()
		}
	}

	······
	// 获取有可运行gorountine队列的p，这里还没有，所以for完后，runnablePs=nil
	var runnablePs *p
	for i := nprocs - 1; i >= 0; i-- {
		p := allp[i]
		if _g_.m.p.ptr() == p {		// 当遍历到allp[0]时，是continue
			continue
		}
		p.status = _Pidle		// 更改allp中的p状态为 _Pidle
		if runqempty(p) {		
			pidleput(p)			// 维护p的一个链表，并且更新schedt.pidle = ，这里最后的结果是schedt.pidle = allp[1]
		} else {
			p.m.set(mget())
			p.link.set(runnablePs)
			runnablePs = p
		}
	}
	stealOrder.reset(uint32(nprocs))
    // 修改全局变量 gomaxprocs = nprocs
	var int32p *int32 = &gomaxprocs // make compiler check that gomaxprocs is an int32
	atomic.Store((*uint32)(unsafe.Pointer(int32p)), uint32(nprocs))
	return runnablePs // 返回nil
}

```

​		函数省略了跟调度不太相关的代码。这个函数主要是初始化`nprocs`个p，用全局变量`allp`来维护一个p的数组，并且用allp[0]与m0进行绑定。至此，系统线程通过段寄存器绑定m0，其中又关联着g0，而m0又与p进行了绑定，因此g0 、m0、p 以及  thread 都关联在一起； m 和 p 中都维护着一个链表，指向上一个m和p，全局变量`schedt`中，也用变量`pidle` 和` npidle `维护着空闲的p以及数量。



