# **goroutine**

## 什么是gorountine

> ​		They're called *goroutines* because the existing terms—threads, coroutines, processes, and so on—convey inaccurate connotations. A goroutine has a simple model: it is a function executing concurrently with other goroutines in the same address space. It is lightweight, costing little more than the allocation of stack space. And the stacks start small, so they are cheap, and grow by allocating (and freeing) heap storage as required.
>
> Goroutines are multiplexed onto multiple OS threads so if one should block, such as while waiting for I/O, others continue to run. Their design hides many of the complexities of thread creation and management.
>
> --Effective Go

从以上描述能知道gorountine具有的一些特点：

1. gorountine是能跟其他gorountine在一个地址空间里并发执行的function 
2. 非常轻量，仅仅需要分配栈空间
3. 创建成本低，初始栈空间很小并且能根据需要在堆上增长
4. 实质是对thread的封装，对用户隐藏thread的创建和管理

## gorountine与thread的比较

gorountine跟thread 可以从三个方面来比较：内存消耗、创建和销毁、切换。

- 内存消耗：创建gorountine仅需要2k的大小，并且gorountine的栈空间不足时会自动扩容；另一方面，thread的创建至少需要1Mb，是gorountine的500多倍，除此之外还需要一个叫`guard page`的内存区域作为屏障来隔离其他的thread。也因为这样，在一个web服务器上，每个请求都创建一个gorountine去处理是完全没问题的，但如果每个请求都用一个thread去处理的话，服务器总是会内存溢出的，这不仅仅是java的问题，也是其他用thread作为并发方式的语言会遇到的问题
- 创建和销毁：thread的创建和销毁都需要占用系统资源并且在完成后才会返还，相应的解决办法就是维护一个线程池；而对于gorountine，它的创建和销毁都是在runtime(用户代码层面)上，因此需要的成本非常低，另外一方面，go是不能手动管理gorountine
- 切换：thread的调度由系统管理，是抢占式的，更重要的是，在切换thread时，调度器需要保存所有在当前thread运行时的寄存器的值，而这给thread的快速切换带来非常大的影响。gorountine的调度主要是协作式的，切换gorountine的成本很低，只需要保存三个在当前gorountine运行时的寄存器的值：PC、SP、BP。不过按照前面的描述，一般在程序运行时，gorountine的数量是很多的，那这会影响gorountine之间的切换速度吗？其实并不会，原因主要有两个：1、切换只发生在运行中的gorountine；2、当今调度器做切换的复杂度是O(1)，这说明不管是gorountine还是thread，数量跟切换时间都是没有关系的

# **scheduler**

## M:N 的两级线程模型

​	从前面的描述来看，gorountine是对thread的封装，它比thread的层级更高一些，更"亲近"用户。具体来说，go的Runtime在程序启动时，会创建M个thread，而之后创建的N个gorountine都会基于M个thread去运行，也就是说N个gorountine是运行在M个thread之上的；另一方面，thread在运行时都有其对应的CPU寄存器的值，在切换到其他thread时，会将CPU寄存器的值保存到内存中；同样的，在go中thread使用的CPU寄存器的值（PC、SP、BP）取自于当前运行的gorountine，在切换gorountine时，会将这些CPU寄存器的值保存到内存中。以上这些特点实现了一个多对多（M:N ）的两级线程模型。

## 什么是scheduler

​	调度是一个分配使用权的过程（选择和切换）。系统内核负责thread的调度，分配thread对处理器的使用权；基于M:N 模型可以知道gorountine是运行在thread之上，在同一个时刻，一个thread只能运行一个gorountine，那么这自然也需要有一个"gorountine调度器"负责调度gorountine，来分配thread的使用权，而scheduler就是充当gorountine调度器这样的一个角色。另外下面这段参考资料也可以帮助我们宏观的了解到调度器的工作流程

> 所谓的**对goroutine的调度，是指程序代码按照一定的算法在适当的时候挑选出合适的goroutine并放到CPU上去运行的过程**，这些负责对goroutine进行调度的程序代码我们称之为**goroutine调度器**。用极度简化了的伪代码来描述goroutine调度器的工作流程大概是下面这个样子：
>
> ```
> // 程序启动时的初始化代码
> ......
> for i := 0; i < N; i++ { // 创建N个操作系统线程执行schedule函数
>      create_os_thread(schedule) // 创建一个操作系统线程执行schedule函数
> }
> 
> //schedule函数实现调度逻辑
> func schedule() {
>     for { //调度循环
>           // 根据某种算法从M个goroutine中找出一个需要运行的goroutine
>           g := find_a_runnable_goroutine_from_M_goroutines()
>           run_g(g) // CPU运行该goroutine，直到需要调度其它goroutine才返回
>           save_status_of_g(g) // 保存goroutine的状态，主要是寄存器的值
>       }
> }
> ```
>
> --阿波张gorountine调度器系列教程

# 	调度器数据结构

## 		数据结构的抽象

​		正如前面描述的，不管是thread还是gorountine的切换，其实质都是将上下文（CPU寄存器的值）保存到内存中，并把原先保存在内存中的值放到CPU寄存器上而恢复运行。因此，在这个基础上，每个gorountine都需要用一个数据结构来保存对应的CPU寄存器的值以及一些状态信息。在Go中，这个数据结构是一个名叫`g`的结构体，它的每个实例都对应一个gorountine。而Go也用一个`m`结构体抽象出thread，同样的，每个工作线程都会对应一个`m`的实例，其中记录着thread的状态信息等。此外，Go还用一个代表调度器的`schedt`结构体来管理全局，每个Go程序只有一个``schedt``实例并且被定义成一个全局变量。

​		在早期的Go版本中，只有以上`g`、`m`、`schedt`参与调度，每次`m`都必须在加了全局锁后，才从全局队列中获取可运行的`g`来运行，这样在高并发时，效率可能不尽人意。经过优化后，Go引入了一个叫`p`的结构体，将全局队列中的`g`分散到一定数量的`p`中，由`p`去维护这些`g`。`m`通过绑定一个`p`后，就能间接的获取到可运行的`g`而不需要像之前那样每次获取`g`都需要加锁，这样就解决了全局锁带来的问题而具有可观的并发能力。

​		你可能会想为什么不直接把全局队列中的`g`分散到`m`上？不引入`p`而是由m去维护一个局部gorountine可运行队列，这样`m`也能不加锁就能获取到`g`。但这样也会有问题，设想下`m`在运行着一个gorountine时因为某些情况阻塞了（例如系统调用），那么该`m`的局部gorountine可运行队列中的gorountine都将被阻塞。但是通过引入`p`后，就把`m`和局部gorountine可运行队列解耦了。那么再当`m`阻塞时（`m`身上带着一个gorountine），`p`就可以带着gorountine队列去绑定新的`m`!

​		更重要的是，因为`m`绑定`p`后，才能获得`g`运行，所以可以通过控制`p`的数量来间接限制`m`的数量（也就是thread的数量）。大家都知道thread确实非常“重”，所以Go程序会尽可能减少和限制thread的运行数量，否则就违背初衷了。而Go在程序启动时，默认生成`p`的数量等于 CPU 的核心数目。

下面是`g`、`p`、`m`和`schedt`之间的关系图:





​		图中比较简洁的表示了它们之间的关系，其中正在运行gorountine的`m`阻塞时，`p`可与它解绑，依附在新的`m`上运行。但是这些`g`、`p`、`m`都是Go层面的调度角色，`m`是Go对thread的抽象，真正在干活的还是系统线程，正如前所说的，每个系统线程都会对应一个`m`，那么它们是怎么对应起来的呢？其实这里是用到了线程本地存储（Thread Local Storage，TLS），简单的说，因为系统线程都是私有的上下文（寄存器的值），通过寄存器（fs段寄存器）就可以实现私有变量的存储，这样就将`m`和系统线程一一对应了起来，当系统线程找到了`m`，自然也可以通过`m`找到`p`和`g`。[有关线程本地存储的知识，阿波张的文章写得很好，值得详细一看](https://www.cnblogs.com/abozhang/p/10800332.html)。

## 数据结构的具体定义

​		以上提及的结构体的定义全部位于Go源代码路径下的`runtime/runtime2.go`

### **g结构体**

```go
type g struct {
	// Stack parameters.
	// stack describes the actual stack memory: [stack.lo, stack.hi).
	// stackguard0 is the stack pointer compared in the Go stack growth prologue.
	// It is stack.lo+StackGuard normally, but can be StackPreempt to trigger a preemption.
	// stackguard1 is the stack pointer compared in the C stack growth prologue.
	// It is stack.lo+StackGuard on g0 and gsignal stacks.
	// It is ~0 on other goroutine stacks, to trigger a call to morestackc (and crash).
	stack       stack   // offset known to runtime/cgo
	stackguard0 uintptr // offset known to liblink
	stackguard1 uintptr // offset known to liblink

	_panic         *_panic // innermost panic - offset known to liblink
	_defer         *_defer // innermost defer
	m              *m      // current m; offset known to arm liblink
	sched          gobuf
	syscallsp      uintptr        // if status==Gsyscall, syscallsp = sched.sp to use during gc
	syscallpc      uintptr        // if status==Gsyscall, syscallpc = sched.pc to use during gc
	stktopsp       uintptr        // expected sp at top of stack, to check in traceback
	param          unsafe.Pointer // passed parameter on wakeup
	atomicstatus   uint32
	stackLock      uint32 // sigprof/scang lock; TODO: fold in to atomicstatus
	goid           int64
	schedlink      guintptr
	waitsince      int64      // approx time when the g become blocked
	waitreason     waitReason // if status==Gwaiting
	preempt        bool       // preemption signal, duplicates stackguard0 = stackpreempt
	paniconfault   bool       // panic (instead of crash) on unexpected fault address
	preemptscan    bool       // preempted g does scan for gc
	gcscandone     bool       // g has scanned stack; protected by _Gscan bit in status
	gcscanvalid    bool       // false at start of gc cycle, true if G has not run since last scan; TODO: remove?
	throwsplit     bool       // must not split stack
	raceignore     int8       // ignore race detection events
	sysblocktraced bool       // StartTrace has emitted EvGoInSyscall about this goroutine
	sysexitticks   int64      // cputicks when syscall has returned (for tracing)
	traceseq       uint64     // trace event sequencer
	tracelastp     puintptr   // last P emitted an event for this goroutine
	lockedm        muintptr
	sig            uint32
	writebuf       []byte
	sigcode0       uintptr
	sigcode1       uintptr
	sigpc          uintptr
	gopc           uintptr         // pc of go statement that created this goroutine
	ancestors      *[]ancestorInfo // ancestor information goroutine(s) that created this goroutine (only used if debug.tracebackancestors)
	startpc        uintptr         // pc of goroutine function
	racectx        uintptr
	waiting        *sudog         // sudog structures this g is waiting on (that have a valid elem ptr); in lock order
	cgoCtxt        []uintptr      // cgo traceback context
	labels         unsafe.Pointer // profiler labels
	timer          *timer         // cached timer for time.Sleep
	selectDone     uint32         // are we participating in a select and did someone win the race?

	// Per-G GC state

	// gcAssistBytes is this G's GC assist credit in terms of
	// bytes allocated. If this is positive, then the G has credit
	// to allocate gcAssistBytes bytes without assisting. If this
	// is negative, then the G must correct this by performing
	// scan work. We track this in bytes to make it fast to update
	// and check for debt in the malloc hot path. The assist ratio
	// determines how this corresponds to scan work debt.
	gcAssistBytes int64
}

// Stack 描述gorountine的运行时的栈
type stack struct {
	lo uintptr
	hi uintptr
}


// gobuf 主要是在切换时，保存寄存器的值
type gobuf struct {
	// The offsets of sp, pc, and g are known to (hard-coded in) libmach.
	//
	// ctxt is unusual with respect to GC: it may be a
	// heap-allocated funcval, so GC needs to track it, but it
	// needs to be set and cleared from assembly, where it's
	// difficult to have write barriers. However, ctxt is really a
	// saved, live register, and we only ever exchange it between
	// the real register and the gobuf. Hence, we treat it as a
	// root during stack scanning, which means assembly that saves
	// and restores it doesn't need write barriers. It's still
	// typed as a pointer so that any other writes from Go get
	// write barriers.
	sp   uintptr	// 存储 rsp 寄存器的值
	pc   uintptr	 // 存储 rip 寄存器的值
	g    guintptr	// 指向 goroutine
	ctxt unsafe.Pointer
	ret  sys.Uintreg	// 保存系统调用的返回值
	lr   uintptr
	bp   uintptr // for GOEXPERIMENT=framepointer
}
```

### 		**m结构体**

```go
type m struct {
	g0      *g     // goroutine with scheduling stack
	morebuf gobuf  // gobuf arg to morestack
	divmod  uint32 // div/mod denominator for arm - known to liblink

	// Fields not known to debuggers.
	procid        uint64       // for debuggers, but offset not hard-coded
	gsignal       *g           // signal-handling g
	goSigStack    gsignalStack // Go-allocated signal handling stack
	sigmask       sigset       // storage for saved signal mask
	tls           [6]uintptr   // thread-local storage (for x86 extern register)
	mstartfn      func()
	curg          *g       // current running goroutine
	caughtsig     guintptr // goroutine running during fatal signal
	p             puintptr // attached p for executing go code (nil if not executing go code)
	nextp         puintptr
	oldp          puintptr // the p that was attached before executing a syscall
	id            int64
	mallocing     int32
	throwing      int32
	preemptoff    string // if != "", keep curg running on this m
	locks         int32
	dying         int32
	profilehz     int32
	spinning      bool // m is out of work and is actively looking for work
	blocked       bool // m is blocked on a note
	newSigstack   bool // minit on C thread called sigaltstack
	printlock     int8
	incgo         bool   // m is executing a cgo call
	freeWait      uint32 // if == 0, safe to free g0 and delete m (atomic)
	fastrand      [2]uint32
	needextram    bool
	traceback     uint8
	ncgocall      uint64      // number of cgo calls in total
	ncgo          int32       // number of cgo calls currently in progress
	cgoCallersUse uint32      // if non-zero, cgoCallers in use temporarily
	cgoCallers    *cgoCallers // cgo traceback if crashing in cgo call
	park          note
	alllink       *m // on allm
	schedlink     muintptr
	mcache        *mcache
	lockedg       guintptr
	createstack   [32]uintptr // stack that created this thread.
	lockedExt     uint32      // tracking for external LockOSThread
	lockedInt     uint32      // tracking for internal lockOSThread
	nextwaitm     muintptr    // next m waiting for lock
	waitunlockf   func(*g, unsafe.Pointer) bool
	waitlock      unsafe.Pointer
	waittraceev   byte
	waittraceskip int
	startingtrace bool
	syscalltick   uint32
	thread        uintptr // thread handle
	freelink      *m      // on sched.freem

	// these are here because they are too large to be on the stack
	// of low-level NOSPLIT functions.
	libcall   libcall
	libcallpc uintptr // for cpu profiler
	libcallsp uintptr
	libcallg  guintptr
	syscall   libcall // stores syscall parameters on windows

	vdsoSP uintptr // SP for traceback while in VDSO call (0 if not in call)
	vdsoPC uintptr // PC for traceback while in VDSO call

	dlogPerM

	mOS
}
```

### p结构体

```go
type p struct {
	id          int32
	status      uint32 // one of pidle/prunning/...
	link        puintptr
	schedtick   uint32     // incremented on every scheduler call
	syscalltick uint32     // incremented on every system call
	sysmontick  sysmontick // last tick observed by sysmon
	m           muintptr   // back-link to associated m (nil if idle)
	mcache      *mcache
	raceprocctx uintptr

	deferpool    [5][]*_defer // pool of available defer structs of different sizes (see panic.go)
	deferpoolbuf [5][32]*_defer

	// Cache of goroutine ids, amortizes accesses to runtime·sched.goidgen.
	goidcache    uint64
	goidcacheend uint64

	// Queue of runnable goroutines. Accessed without lock.
	runqhead uint32
	runqtail uint32
	runq     [256]guintptr
	// runnext, if non-nil, is a runnable G that was ready'd by
	// the current G and should be run next instead of what's in
	// runq if there's time remaining in the running G's time
	// slice. It will inherit the time left in the current time
	// slice. If a set of goroutines is locked in a
	// communicate-and-wait pattern, this schedules that set as a
	// unit and eliminates the (potentially large) scheduling
	// latency that otherwise arises from adding the ready'd
	// goroutines to the end of the run queue.
	runnext guintptr

	// Available G's (status == Gdead)
	gFree struct {
		gList
		n int32
	}

	sudogcache []*sudog
	sudogbuf   [128]*sudog

	tracebuf traceBufPtr

	// traceSweep indicates the sweep events should be traced.
	// This is used to defer the sweep start event until a span
	// has actually been swept.
	traceSweep bool
	// traceSwept and traceReclaimed track the number of bytes
	// swept and reclaimed by sweeping in the current sweep loop.
	traceSwept, traceReclaimed uintptr

	palloc persistentAlloc // per-P to avoid mutex

	_ uint32 // Alignment for atomic fields below

	// Per-P GC state
	gcAssistTime         int64    // Nanoseconds in assistAlloc
	gcFractionalMarkTime int64    // Nanoseconds in fractional mark worker (atomic)
	gcBgMarkWorker       guintptr // (atomic)
	gcMarkWorkerMode     gcMarkWorkerMode

	// gcMarkWorkerStartTime is the nanotime() at which this mark
	// worker started.
	gcMarkWorkerStartTime int64

	// gcw is this P's GC work buffer cache. The work buffer is
	// filled by write barriers, drained by mutator assists, and
	// disposed on certain GC state transitions.
	gcw gcWork

	// wbBuf is this P's GC write barrier buffer.
	//
	// TODO: Consider caching this in the running G.
	wbBuf wbBuf

	runSafePointFn uint32 // if 1, run sched.safePointFn at next safe point

	pad cpu.CacheLinePad
}
```

### shedt结构体

```go
type schedt struct {
	// accessed atomically. keep at top to ensure alignment on 32-bit systems.
	goidgen  uint64
	lastpoll uint64

	lock mutex

	// When increasing nmidle, nmidlelocked, nmsys, or nmfreed, be
	// sure to call checkdead().

	midle        muintptr // idle m's waiting for work
	nmidle       int32    // number of idle m's waiting for work
	nmidlelocked int32    // number of locked m's waiting for work
	mnext        int64    // number of m's that have been created and next M ID
	maxmcount    int32    // maximum number of m's allowed (or die)
	nmsys        int32    // number of system m's not counted for deadlock
	nmfreed      int64    // cumulative number of freed m's

	ngsys uint32 // number of system goroutines; updated atomically

	pidle      puintptr // idle p's
	npidle     uint32
	nmspinning uint32 // See "Worker thread parking/unparking" comment in proc.go.

	// Global runnable queue.
	runq     gQueue
	runqsize int32

	// disable controls selective disabling of the scheduler.
	//
	// Use schedEnableUser to control this.
	//
	// disable is protected by sched.lock.
	disable struct {
		// user disables scheduling of user goroutines.
		user     bool
		runnable gQueue // pending runnable Gs
		n        int32  // length of runnable
	}

	// Global cache of dead G's.
	gFree struct {
		lock    mutex
		stack   gList // Gs with stacks
		noStack gList // Gs without stacks
		n       int32
	}

	// Central cache of sudog structs.
	sudoglock  mutex
	sudogcache *sudog

	// Central pool of available defer structs of different sizes.
	deferlock mutex
	deferpool [5]*_defer

	// freem is the list of m's waiting to be freed when their
	// m.exited is set. Linked through m.freelink.
	freem *m

	gcwaiting  uint32 // gc is waiting to run
	stopwait   int32
	stopnote   note
	sysmonwait uint32
	sysmonnote note

	// safepointFn should be called on each P at the next GC
	// safepoint if p.runSafePointFn is set.
	safePointFn   func(*p)
	safePointWait int32
	safePointNote note

	profilehz int32 // cpu profiling rate

	procresizetime int64 // nanotime() of last change to gomaxprocs
	totaltime      int64 // ∫gomaxprocs dt up to procresizetime
}

```

### 重要的全局变量

```

```

